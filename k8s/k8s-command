
=>list and  watch with sorting the cpu uses of the pod :
$ watch -n 1 "kubectl top pod -n <my-namespace> --sort-by=cpu"


=>list and  watch with  the HPA uses of the perticaulr namespace :
$ watch -n 1 "kubectl get hpa -n <my-namespace> "



=>find the error from the log of pod:
kubectl logs <my-podname > -n <my-namespace> | grep "error"


watch -n 1 "kubectl describe  hpa <my-hpa-name> -n deliveryservice"

=========
https://metallb.universe.tf/usage/    // loadbalancer



This will display the node name where the pod is scheduled:
$ kubectl get pod -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name --all-namespaces


$ helm get values RELEASE_NAME [flags]

======
$ helm get values metallb -n metallb-system

$ helm list --all-namespaces

======

Logs:
$ kubectl logs -f <my-podname> -n <my-namespace> --tail 100

Priting the logs output in logs.txt file:
$ kubectl logs -f <my-podname> -n <my-namespace> > logs.txt

$ cat logs.txt | grep SendMessage                      # check the pattern lets say "SendMessage"




==================
traffic monitoring :
=====================
This command retrieves the network I/O statistics (received bytes and transmitted bytes) for a specific container within a pod in a Kubernetes namespace using JSONPath format:
$ kubectl get pod <pod name> -o jsonpath='{.status.containerStatuses[0].network.interfaces[0].rxBytes}{"\t"}{.status.containerStatuses[0].network.interfaces[0].txBytes}' -n <namespace>

This command allows you to execute the netstat -antp command inside the container named <my-pod> in the <default> namespace, providing you with information about network connections and listening ports within the container.
$ kubectl exec -it <my-pod> -n <default> -- netstat -antp

kubectl top pods -n default -p <my-pod>





==========
Troubleshoot Error:
=================
$ kubectl get events --field-selector type=Warning --all-namespaces        # all the pod with warning .
$ kubectl get nodes -o wide --label-columns topology.kubernetes.io/zone    # Detailed information of master and worker machine with iso image and all.





===========
Helm:
helm list -n <namespace>                                                            # to list the release name 
helm get manifest loyaltygatewayservice -n gatewayservice                           # to check the manifest 

helm upgrade loyaltygatewayservice -f <modified-manifest-file> -n gatewayservice














============
kubectl get service my-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}'  

https://blog.getambassador.io/externaltrafficpolicy-local-on-kubernetes-e66e498212f9



Scale down/up :
===============
kubectl scale deployment loyaltytransactionapi --replicas=1 -n transactionapi

kubectl scale deployment loyaltytransactionapi --replicas=1 -n transactionapi
kubectl scale statefulset <> --replicas=1 -n transactionapi



labling the node for resource managemnet 
==============
kubectl get nodes --show-labels
kubectl label nodes 172.28.53.57  type=app --overwrite

Forcefully delete ns with state Terminating:
============================================
NS=`kubectl get ns |grep Terminating | awk 'NR==1 {print $1}'` && kubectl get namespace "$NS" -o json   | tr -d "\n" | sed "s/\"finalizers\": \[[^]]\+\]/\"finalizers\": []/"   | kubectl replace --raw /api/v1/namespaces/$NS/finalize -f -

helm uninstall --namespace wallet wallet-api

Forcefully delete pod:
======================
kubectl delete pod <PODNAME> --grace-period=0 --force --namespace <NAMESPACE>

Forcefully delete PV and PVC :
===============================
kubectl patch pv <persistentvolume-name> -p '{"metadata":{"finalizers":null}}'

kubectl patch pvc <persistentvolumeclaim-name> -n <namespace> -p '{"metadata":{"finalizers":null}}'

$ kubectl patch pv pvc-40be33e2-8f93-410a-b3bf-00f5d03501d5 -p '{"metadata":{"finalizers":null}}'
$ kubectl patch pvc elasticsearch-data-devops-es-data-1 -n elastic-system -p '{"metadata":{"finalizers":null}}'



Delete the pods with spesific status :
=======================================
kubectl delete pod -n identity-server `kubectl get pods -n identity-server | awk '$3 == "ContainerStatusUnknown" {print $1}'`
    	  


Copy file inside of conatiner:
==============================		  
kubectl cp /path/to/local/file.txt <pod-name>:/path/inside/pod/
		  	  
	  
# Create a tar archive of the local text file
tar czf /tmp/example.tar.gz -C /path/to/local/ example.txt

# Copy the tar archive to the first location inside the pod
kubectl cp /tmp/example.tar.gz my-pod:/path/inside/pod/location1/

# Execute a command inside the pod to extract the tar archive to the second location
kubectl exec my-pod -- tar xzf /path/inside/pod/location1/example.tar.gz -C /path/inside/pod/location2/
	  

Context Command:
================
kubectl config get-contexts             //To view all the contenxs
kubectl congif use-contexts  <context_name>           //To use perticular context.
kubectl config current-contexts                       // to view the correct contexts

k run <podname> --image=httpd:2.4.41-alpine --dry-run=clinet -o yaml            //it will do dry run and then print in yaml format  





kubectl delete pod -n managementapp `kubectl get pods -n managementapp | awk '$3 == "Evicted" {print $1}'`



kubectl rollout restart deployment loyaltydeliveryservice -n deliveryservice

kubectl rollout restart deployment development-ids -n identity-server

kubectl rollout restart deployment development-idsapi -n identity-server

kubectl rollout restart deployment development-idsui -n identity-server
